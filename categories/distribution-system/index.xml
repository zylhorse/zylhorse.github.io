<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Distribution System on zylhorse blog</title>
    <link>https://zylhorse.github.io/categories/distribution-system/</link>
    <description>Recent content in Distribution System on zylhorse blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-CN</language>
    <lastBuildDate>Fri, 23 Apr 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://zylhorse.github.io/categories/distribution-system/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>分布式监控系统-Tracing、Metrics、Logging</title>
      <link>https://zylhorse.github.io/blog/distribution-system/%E5%88%86%E5%B8%83%E5%BC%8F%E7%9B%91%E6%8E%A7%E7%B3%BB%E7%BB%9F/</link>
      <pubDate>Fri, 23 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>https://zylhorse.github.io/blog/distribution-system/%E5%88%86%E5%B8%83%E5%BC%8F%E7%9B%91%E6%8E%A7%E7%B3%BB%E7%BB%9F/</guid>
      <description>Metrics、Tracing、Logging都是针对全局的，肯定会有重叠的部分。因此思考 使用维恩图(Venn diagram)描述三个概念的定义，用以确定三者不同的部分:
Metrics: 统计指标是可计量的，它们是在一段时间内组成单个逻辑gauge，counter，histogram的原子。 例如：
队列的当前深度可以被定义为一个gauge，根据last-writer-win原则更新总计
HTTP请求的数量可以被定义为计数器，根据简单的累加更新总计
请求时长可以被定义为一个柱状图，更新总计到时间段并产生统计摘要
Logging：用于处理离散(偶发)事件。
例如:
应用程序的debug/error信息通过syslog的滚动文件描述符，转存到ELK中
审计踪迹事件通过Kafka，存储到像BigTable这样的数据库中
特定请求的元数据信息，从服务请求中剥离，并发送给异常收集服务器，如:NewRelic
Tracing: 用于处理请求范围内的信息。任何数据或者元数据都可以绑定到系统的单个事务对象的生命周期上。
例如:
出站RPC到远程服务请求时长
发送到数据库的SQL查询文本
入站HTTP请求的关联ID
根据上述定义，可以标记重叠部分: 对于cloud-native应用来说，大部分instrumentation的典型特征是以请求范围结束，因此讨论tracing的上下文是有意义的。 但是我们可以观察到不是所有的instrumentation都绑定到请求的生命周期上：e.g. 逻辑组件的诊断信息或进程生命周期信息，与任何离散请求是正交关系。 因此不是所有(至少未经处理)的metrics或logs信息都可以硬塞进tracing系统。
我们应该意识到metrics统计数据，对应用监控有很大帮助，比如：Prometheus生态，可以展现应用的实时监控视图。 相反的，如果将metrics信息硬塞进logging系统处理，会让我们丢失一些特性。
至此，我们可以对现有系统进行分类。如：
Prometheus系统，起始是作为专一的metrics系统，慢慢的可能会向tracing系统发展，从而进行请求范围的metrics。但是不会向logging系统领域深入。
ELK生态提供日志的记录、滚动和聚合，也在其它领域不停的积累，并慢慢的集成进来。
在这三个领域中Metrics需要最少的管理资源，Logging的资源负荷往往是递增的。
由此我们可以形成容量或操作开销梯度图，又低到高，可以看出Tracing的开销居于前两者之间：</description>
    </item>
    
    <item>
      <title>服务治理-限流</title>
      <link>https://zylhorse.github.io/blog/distribution-system/%E9%99%90%E6%B5%81/</link>
      <pubDate>Sun, 08 Nov 2020 00:00:00 +0000</pubDate>
      
      <guid>https://zylhorse.github.io/blog/distribution-system/%E9%99%90%E6%B5%81/</guid>
      <description>概念 当接口的访问频率或者并发请求数量超过系统的承受范围时， 需要考虑限流来保证接口的可用性；
解决方案 通常策略是拒绝多余的访问或者让多余的访问排队等待；
令牌桶 控制网络上数据的数目， 允许突发流量。系统会以一定的速度向桶里放入令牌， 如果有请求要被处理， 就去桶里面取一块令牌。 当桶里没有令牌可取时， 则拒绝请求。
import (&amp;quot;sync&amp;quot;&amp;quot;time&amp;quot;)// This implement the token buckettype TokenBucket struct {rate int64 // the number of tokens are putted to bucket per secondcapacity int64 // the capacity of buckettokens int64 // current number of tokens in bucketlastTimestamp int64 // last timestamp of putted tokens to bucket.lock sync.Mutex}func (l *TokenBucket) Allow() bool {l.</description>
    </item>
    
    <item>
      <title>服务治理-服务注册和发现</title>
      <link>https://zylhorse.github.io/blog/distribution-system/%E6%9C%8D%E5%8A%A1%E5%8F%91%E7%8E%B0/</link>
      <pubDate>Mon, 11 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>https://zylhorse.github.io/blog/distribution-system/%E6%9C%8D%E5%8A%A1%E5%8F%91%E7%8E%B0/</guid>
      <description>概述 </description>
    </item>
    
    <item>
      <title>服务治理-服务的熔断和降级</title>
      <link>https://zylhorse.github.io/blog/distribution-system/%E7%86%94%E6%96%AD%E5%92%8C%E9%99%8D%E7%BA%A7/</link>
      <pubDate>Mon, 11 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>https://zylhorse.github.io/blog/distribution-system/%E7%86%94%E6%96%AD%E5%92%8C%E9%99%8D%E7%BA%A7/</guid>
      <description>Hystrix 和 Sentinel</description>
    </item>
    
    <item>
      <title>服务治理-负载均衡</title>
      <link>https://zylhorse.github.io/blog/distribution-system/%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1/</link>
      <pubDate>Sun, 20 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>https://zylhorse.github.io/blog/distribution-system/%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1/</guid>
      <description> 随机 加权随机 哈希 最小压力  </description>
    </item>
    
    <item>
      <title>分布式事务</title>
      <link>https://zylhorse.github.io/blog/distribution-system/%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1/</link>
      <pubDate>Fri, 11 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>https://zylhorse.github.io/blog/distribution-system/%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1/</guid>
      <description>概述  分布式事务： 分布式事务涉及到多组操作， 将一个事务的概念扩大到多个分布式操作上 分布式事务的参与者，资源服务器，事务管理器分别位于分布式系统的不同节点上  分布式事务产生原因  service 产生多个节点 resource 产生多个节点  </description>
    </item>
    
    <item>
      <title>分布式锁</title>
      <link>https://zylhorse.github.io/blog/distribution-system/%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81/</link>
      <pubDate>Wed, 02 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>https://zylhorse.github.io/blog/distribution-system/%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81/</guid>
      <description>分布式锁 应用场景  多节点并发的对数据进行操作； 多节点并发的执行同一个任务， 请求同一接口；  分布式锁条件  在分布式系统环境下， 一个资源同时只能被一个线程执行； 高可用的获取锁和释放锁 高性能的获取锁和释放锁 具备可重入性， 同一线程可以并发使用； 具备锁失效机制，不会死锁 非阻塞特性， 即获取锁失败需要返回失败；  分布式锁实现  redis分布式锁 mutext.go:  package redsync// lockfunc (m *Mutex) Lock() error {// generate mutex identifier, used for unlock and extendvalue, err := m.genValueFunc()if err != nil {return err}// acquire the lock and try some times when failed.for i := 0; i &amp;lt; m.</description>
    </item>
    
    <item>
      <title>分布式一致性</title>
      <link>https://zylhorse.github.io/blog/distribution-system/%E5%88%86%E5%B8%83%E5%BC%8F%E4%B8%80%E8%87%B4%E6%80%A7/</link>
      <pubDate>Mon, 30 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>https://zylhorse.github.io/blog/distribution-system/%E5%88%86%E5%B8%83%E5%BC%8F%E4%B8%80%E8%87%B4%E6%80%A7/</guid>
      <description>一致性算法 算法目的是让整个集群的结点对某个值的变更达成一致
 Paxos 算法目的是让整个集群的结点对某个值的变更达成一致。Paxos 算法(强一致性算法)属于多数派——大多数的决定会成个整个集群的统一决定。任何一个点都可以提出要修改某个数据的提案，是否通过这个提案取决于这个集群中是否有超过半数的结点同意（所以 Paxos 算法需要集群中的结点是单数）；  这个算法有两个大阶段，四个小阶段（Paxos 有 Proposer 和 Acceptor 两个角色）Prepare proposer 提出一个提案，编号为 N ，此 N 大于这个 proposer 之前提出提案编号。请求 acceptors 的 quorum 接受。Promise 如果 N 大于此 acceptor 之前接受的任何提案编号则接受，否则拒绝Accept 如果达到了多数派， Proposer 会发出 accept 请求，此请求包含提案编号 N，以及提案内容Accepted 如果此 acceptor 在此期间没有收到任何编号大于 N 的提案，则接受此提案内容，否则忽略Raft 是简化版的 Paxos。Raft 划分成三个子问题：一是Leader Election；二是 Log Replication；三是 Safety。Raft 定义了三种角色 Leader、Follower、Candidate，最开始大家都是 Follower，当 Follower 监听不到 Leader，就可以自己成为 Candidate，发起投票。  </description>
    </item>
    
    <item>
      <title>分布式系统</title>
      <link>https://zylhorse.github.io/blog/distribution-system/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/</link>
      <pubDate>Thu, 19 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>https://zylhorse.github.io/blog/distribution-system/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/</guid>
      <description>概念  建立在网络上的软件系统 若干独立系统的集合  特性  分布性： 分布式系统由多个节点组成， 地域上分散； 自治性： 分布式系统的每个节点都包含自己的处理器和内存， 具备独立处理数据能力； 并行性： 一个完整的任务可以划分成若干子任务， 运行在分布式系统的多个节点上； 全局性： 分布式系统中的节点可以互相通信及系统间的相互调用；  CAP   一致性（consistency）： 分布式系统中的所有数据备份，在同一时刻是否是同样的值（分为弱、强、最终一致性）
  可用性（availability）： 在集群中某一个节点故障后， 集群整体是否还能响应客户端的读写请求（数据高可用性）
  分区容忍性（partition tolerance）： 提供如果在时限内不能达成数据一致性， 必须就当前操作在一致性和可用性之间做出选择；
  任何一个分布式系统都无法同时满足一致性（consistency)、可用性（availability）和分区容忍性（partition tolerance）；
  大多数场景都需要牺牲强一致性来换取系统的高可用性， 往往只需要保证最终一致性；
  BASE策略  Basically Available（基本可用）：在分布式系统出现不可预知的故障时， 允许损失部分可用性； Soft state（软状态）： 允许系统中数据存在中间状态，并认为该中间状态存在不会影响系统的整体可用性；指的是数据不一致； Eventually consistent（最终一致性）： 系统中所有的数据副本，在一定的时限内，最终能够达到一个一致的状态；  </description>
    </item>
    
    <item>
      <title>CDN内容分发网络工作过程和分类</title>
      <link>https://zylhorse.github.io/blog/distribution-system/cdn/</link>
      <pubDate>Sun, 23 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>https://zylhorse.github.io/blog/distribution-system/cdn/</guid>
      <description>CDN  全称 Content Delivery Network，即内容分发网络 概念始于1996，美国麻省理工学院的研究小组，并于1999年创建CDN服务公司 CDN构建在现有网络基础上，依靠部署在各地的边缘服务器，通过中心平台的负载均衡、内容分发、调度等模块，是用户就近获取内容，降低网络拥塞，挺高用户访问的响应速度和命中率。 CDN的关键技术是内容存储和分发  一些前端开源CDN服务  基于github的cdn BootCDN  CDN基本工作过程 使用CDN可以简化网站的系统维护工作
 运维只需要将网站内容注入到CDN系统，就可以同通过CDN系统部署在各个物理位置的服务器进行全网分发，实现跨运营商、跨地域的用户覆盖 由于CDN将内容推送到网络边缘，大量用户访问被分散到网络边缘  不在构成网站出口、互联互通点的资源挤占 也不需要跨越长距离的IP路由    实现原理 关键字：
 CNAME：即：别名记录。这种记录允许您将多个名字映射到同一台计算机。 GSLB： Global Server Load Balance, 全局负载均衡。 实现在广域网上不同地域服务器间的流量调配，保证使用最佳的服务器服务离自己最近的客户。  今天我们看到的网站系统基本上基于B/S架构，具体流程如下：
 未使用CDN，用户访问网站过程：    用户在自己的浏览器中输入要访问的网站域名 浏览器向本地的DNS服务器请求对该域名解析  本地DNS服务器中如果有缓存该域名的解析结果，则直接响应用户的解析请求 本地DNS服务器中如果没有缓存该域名的解析结果， 则以递归的方式向整个DNS系统请求解析，获得应答后将结果反馈给浏览器   浏览器得到域名的解析结果，就是该域名对应的服务器设备IP 浏览器向服务器请求内容 服务器将用户请求内容传输给浏览器     使用CDN，用户访问网站过程    当用户加载网站上的内容和资源URL时 经过本地DNS系统解析，DNS系统会将域名的解析权交给CNAME指向的CDN专用DNS服务器 CDN的专属DNS服务器将CDN的全局负载均衡设备IP返回给用户 用户向CDN的全局负载均衡设备发起内容URL的访问请求 CDN全局负载均衡设备根据用户IP和用户请求内容URL，选择一台用户所在区域的区域负载均衡设备， 告诉用户向这台设备发起请求 区域负载均衡设备会为用户选择一台合适的缓存服务器提供服务。选择依据：  判断哪台服务器距离用户最近 根据请求URL携带内容，判断哪一台服务器上有用户所需内容 查询当前服务器负载情况，判断哪一台有服务能力。 基于以上条件的综合分析后， 区域负载均衡设备会向全局负载均衡设备返回一台缓存服务器IP   全局负载均衡设备把缓存服务器的IP返回给用户 用户向缓存服务器发起请求，缓存服务器响应用户需求，返回用户请求内容  如果这台缓存服务器没有请求内容，而区域负载均衡设备依然将其分配给用户 这台缓存服务器会向它的上级缓存服务器请求内容，直到追溯到网站的源服务器将内容缓存到自身        CDN基于内容分类 关键字：</description>
    </item>
    
    <item>
      <title>一致性哈希特性和使用场景</title>
      <link>https://zylhorse.github.io/blog/distribution-system/%E4%B8%80%E8%87%B4%E6%80%A7%E5%93%88%E5%B8%8C/</link>
      <pubDate>Sun, 05 May 2019 00:00:00 +0000</pubDate>
      
      <guid>https://zylhorse.github.io/blog/distribution-system/%E4%B8%80%E8%87%B4%E6%80%A7%E5%93%88%E5%B8%8C/</guid>
      <description>概念  一种hash算法 简单说： 移除和添加一台服务器是， 此算法能够尽可能小的改变已存在的服务请求和处理请求服务器之间的映射关系；  特性 考虑到分布式系统每个节点都可能失效， 且新的节点有可能加入进来。 一致性哈希需要保证以下特性
平衡性 hash的结果尽可能的分配到所有的节点去，是的所有节点都能得到利用
分散性  分散性： 客户端请求可能不知道所有的节点存在， 部分客户端将部分节点作为一个完整的hash环；导致同一请求不能映射到同一个节点； 一致性哈希 应降低分散性；  单调性  增加或删除节点，原有的请求应该被映射到原有的节点或新的节点中去； 增加或删除节点 不应造成大量的哈希重定向  工作原理  一致性哈希算法， 将整个hash空间映射成一个虚拟的圆环，取值范围为0-uint32max； 整个空间按顺时针方向组织； 将请求对应的hash映射到圆环上， 沿圆环做顺时针查找， 分配到最近的节点上； 增加节点只会影响新的节点和逆时针节点之间得请求；  </description>
    </item>
    
    <item>
      <title>ZooKeeper工作原理</title>
      <link>https://zylhorse.github.io/blog/distribution-system/zookeeper/</link>
      <pubDate>Fri, 04 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>https://zylhorse.github.io/blog/distribution-system/zookeeper/</guid>
      <description>ZooKeeper ZooKeeper是一个开源的分布式协调服务， 由雅虎创建， 是Google Chubby的开源实现， 现在被拆分为Hadoop的独立子项目。
 ZookKeeper是一个分布式小文件系统，且被设计为高可用。通过选举算法和集群复制来避免单点故障。 因为是基于文件系统设计， 即使所有ZooKeeper节点全部挂掉， 数据也会持久化， 重启服务器后， 可恢复数据。 ZooKeeper通过mvvc（多版本控制）实现乐观锁，节点更新是原子的，所以不是成功就是失败。  功能简介 分布式系统基于ZooKeeper可以实现诸如： 数据发布/订阅、负载均衡、命名服务、分布式协调/通知、集群管理、Master选举、配置维护、名字服务、分布式同步、分布式锁、分布式队列
基本概念 集群角色 一个ZooKeeper集群同一时刻只能有一个Leader， 其他都是Follower或Observer
 Leader Follower Observer  </description>
    </item>
    
  </channel>
</rss>
